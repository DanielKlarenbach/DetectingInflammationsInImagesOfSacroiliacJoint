{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUsage:\\n1. Directory with data sets must be placed in directory named \"input\"\\n2. Input directory must be in the same directory as the .ipynb file with this sript\\n3. Inside each data set directory there must be three directories named: images, labels and masks.\\n4. Filenames of labels and masks files must be the same as image filename.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Usage:\n",
    "1. Directory with data sets must be placed in directory named \"input\"\n",
    "2. Input directory must be in the same directory as the .ipynb file with this sript\n",
    "3. Inside each data set directory there must be three directories named: images, labels and masks.\n",
    "4. Filenames of labels and masks files must be the same as image filename.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.future.graph as skigraph\n",
    "import shutil\n",
    "import pickle\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from kgcnn.literature.GCN import make_model\n",
    "from kgcnn.utils.data import ragged_tensor_from_nested_numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import DBSCAN\n",
    "from kgcnn.utils.learning import LinearLearningRateScheduler\n",
    "from PIL import Image, ImageOps\n",
    "import radiomics\n",
    "import numpy.ma as ma\n",
    "import nrrd\n",
    "import warnings\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script parameterss\n",
    "DATA_SET_DIR_NAME = \"data_set_2\"\n",
    "PYRADIOMICS_FEATURES = [\n",
    "    \"original_firstorder_Mean\",\n",
    "    \"original_firstorder_Variance\",\n",
    "    \"original_glcm_ClusterTendency\",\n",
    "    \"original_glcm_Correlation\",\n",
    "    \"original_ngtdm_Contrast\",\n",
    "    \"original_glrlm_RunEntropy\",\n",
    "    \"original_gldm_DependenceEntropy\",\n",
    "    \"original_gldm_SmallDependenceEmphasis\",\n",
    "    \"original_glrlm_GrayLevelNonUniformity\",\n",
    "    \"original_ngtdm_Busyness\",\n",
    "    \"original_glszm_ZoneEntropy\",\n",
    "    \"original_glszm_SizeZoneNonUniformity\"\n",
    "]\n",
    "\n",
    "# globals\n",
    "DATA_DIR_PATH = f\"./input/{DATA_SET_DIR_NAME}\"\n",
    "IMAGES_DIR_PATH = f\"{DATA_DIR_PATH}/images\"\n",
    "SUPERPIXELS_LABELS_DIR_PATH = f\"{DATA_DIR_PATH}/superpixels_labels\"\n",
    "MASKS_DIR_PATH = f\"{DATA_DIR_PATH}/masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def save_object(filename, obj):\n",
    "    obj_file = open(filename, \"wb\")\n",
    "    pickle.dump(obj, obj_file)\n",
    "    obj_file.close()\n",
    "    \n",
    "def load_object(filename):\n",
    "    obj_file = open(filename, \"rb\")\n",
    "    obj = pickle.load(obj_file)\n",
    "    obj_file.close()\n",
    "    \n",
    "    return obj\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, file_id, rag, image, superpixels_labels, mask):\n",
    "        self.file_id = file_id\n",
    "        self.rag = rag\n",
    "        self.image = image\n",
    "        self.superpixels_labels = superpixels_labels\n",
    "        self.mask = mask\n",
    "\n",
    "class Subgraph:\n",
    "    def __init__(self, rag, graph, middle_superpixel_label, label):\n",
    "        self.rag = rag\n",
    "        self.graph = graph\n",
    "        self.middle_superpixel_label = middle_superpixel_label\n",
    "        self.label = label\n",
    "        self.unnormalized_edge_indices = None\n",
    "        self.normalized_edge_indices = None\n",
    "        self.edges = None\n",
    "        self.nodes = None\n",
    "        \n",
    "def assign_labels(graph):\n",
    "    white_pixels_count = defaultdict(int)\n",
    "    total_pixels_count = defaultdict(int)\n",
    "    \n",
    "    for (i, row) in enumerate(graph.superpixels_labels):\n",
    "        for (j, superpixel_label) in enumerate(row):\n",
    "            total_pixels_count[superpixel_label] += 1\n",
    "            if graph.mask[i][j] == 1:\n",
    "                white_pixels_count[superpixel_label] += 1\n",
    "      \n",
    "    for node in graph.rag:\n",
    "        graph.rag.nodes[node]['label'] = 1.0 if white_pixels_count[node] / total_pixels_count[node] >= 0.65 else 0.0\n",
    "            \n",
    "            \n",
    "def assign_features(graph):\n",
    "    unique_superpixels_labels = np.unique(graph.superpixels_labels)\n",
    "    \n",
    "    for superpixel_label in unique_superpixels_labels:\n",
    "        superpixel_label_mask = (graph.superpixels_labels == superpixel_label).astype(int)\n",
    "        NRRD_DIRECTORY_PATH = \"./output/nrrd\"\n",
    "        os.makedirs(NRRD_DIRECTORY_PATH, exist_ok=True)\n",
    "        \n",
    "        nrrd.write(f\"{NRRD_DIRECTORY_PATH}/{superpixel_label}_image.nrrd\", graph.image)\n",
    "        nrrd.write(f\"{NRRD_DIRECTORY_PATH}/{superpixel_label}_superpixel_label_mask.nrrd\", superpixel_label_mask)\n",
    "        nrrd_image_path = os.path.join(NRRD_DIRECTORY_PATH, str(superpixel_label) + \"_image.nrrd\")\n",
    "        nrrd_superpixel_label_mask_path = os.path.join(NRRD_DIRECTORY_PATH, str(superpixel_label) + \"_superpixel_label_mask.nrrd\")\n",
    "        \n",
    "        extractor = radiomics.featureextractor.RadiomicsFeatureExtractor()\n",
    "        try:             \n",
    "            result = extractor.execute(nrrd_image_path, nrrd_superpixel_label_mask_path)\n",
    "        except Exception as exception:\n",
    "            os.makedirs(f\"./output/nrrd/failed/{graph.filename}\", exist_ok=True)\n",
    "            print(superpixel_label)\n",
    "            print(exception)\n",
    "            print(\"FAILED\")\n",
    "            nrrd.write(f\"{NRRD_DIRECTORY_PATH}/failed/{graph.filename}/{superpixel_label}_superpixel_label_mask.nrrd\", superpixel_label_mask)\n",
    "                \n",
    "        for feature in PYRADIOMICS_FEATURES:\n",
    "            graph.rag.nodes[superpixel_label][feature] = result[feature]\n",
    "    \n",
    "def process_images():\n",
    "    filenames = os.listdir(IMAGES_DIR_PATH)\n",
    "    graphs = list()\n",
    "    os.makedirs(\"./output/expected\", exist_ok=True)\n",
    "    os.makedirs(\"./output/generated\", exist_ok=True)\n",
    "    \n",
    "    for (file_count, filename) in enumerate(filenames, start=1):\n",
    "        print(f\"Processing files: {file_count}/{len(filenames)}\")\n",
    "        \n",
    "        file_id = os.path.splitext(filename)[0]\n",
    "        \n",
    "        try:\n",
    "            image = np.array(ImageOps.grayscale(Image.open(f\"{IMAGES_DIR_PATH}/{filename}\")))\n",
    "            mask = np.array(ImageOps.grayscale(Image.open(f\"{MASKS_DIR_PATH}/{file_id}.bmp\")))\n",
    "            superpixels_labels = np.fromfile(f\"{SUPERPIXELS_LABELS_DIR_PATH}/{file_id}.dat\", dtype=np.dtype((np.int32, image.shape)))[0]\n",
    "        except FileNotFoundError as error: \n",
    "            print(error)\n",
    "            \n",
    "        rag = skigraph.rag_mean_color(image, superpixels_labels)\n",
    "        graphs.append(Graph(file_id, rag, image, superpixels_labels, mask))\n",
    "        assign_labels(graphs[-1])\n",
    "        assign_features(graphs[-1])\n",
    "        \n",
    "        expected = Image.fromarray((mask*255).astype(np.uint8))\n",
    "        expected.save(f\"./output/expected/{file_id}.png\")\n",
    "        \n",
    "    print(\"All files have been processed\")\n",
    "    \n",
    "    return graphs\n",
    "\n",
    "def split_into_subgraphs(graphs):\n",
    "    subgraphs = []\n",
    "    for graph in graphs:\n",
    "        for node in graph.rag.nodes:\n",
    "            nodes = [neighbor for neighbor in graph.rag.neighbors(node)] + [node]\n",
    "            rag = graph.rag.subgraph(nodes)\n",
    "            label = graph.rag.nodes[node]['label']\n",
    "            subgraphs.append(Subgraph(rag, graph, node, label))\n",
    "            \n",
    "    return subgraphs\n",
    "\n",
    "def normalize_edge_indices(edge_indices):\n",
    "    result = []\n",
    "    flat_list = []\n",
    "    for sublist in edge_indices:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "            \n",
    "    flat_list.sort()\n",
    "    flat_list = list(dict.fromkeys(flat_list))\n",
    "    change = {key:value for (value, key) in enumerate(flat_list)}\n",
    "    \n",
    "    for sublist in edge_indices:\n",
    "        temp = []\n",
    "        for item in sublist:\n",
    "            temp.append(change[item])\n",
    "        result.append(temp)    \n",
    "    \n",
    "    return result\n",
    "\n",
    "def prepare_data(subgraphs):\n",
    "    nodes = []\n",
    "    edge_indices = []\n",
    "    edges = []\n",
    "    labels = []\n",
    "    \n",
    "    for subgraph in subgraphs:\n",
    "        node_features = []\n",
    "        \n",
    "        for node in subgraph.rag.nodes:\n",
    "            node_features.append([subgraph.rag.nodes[node][feature] for feature in PYRADIOMICS_FEATURES])\n",
    "            \n",
    "        nodes.append(node_features)\n",
    "        edges.append([1 for edge in subgraph.rag.edges.data()])\n",
    "        edge_indices.append(normalize_edge_indices([list(index) for index in subgraph.rag.edges]))\n",
    "        labels.append(subgraph.label)\n",
    "        \n",
    "        subgraph.nodes = nodes[-1]\n",
    "        subgraph.unnormalized_edge_indices = [list(index) for index in subgraph.rag.edges]\n",
    "        subgraph.normalized_edge_indices = edge_indices[-1]\n",
    "        subgraph.edges = edges[-1]\n",
    "\n",
    "    return nodes, edge_indices, edges, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files: 1/140\n",
      "Processing files: 2/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/7087195078.dat'\n",
      "Processing files: 3/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/6719056356.dat'\n",
      "Processing files: 4/140\n",
      "73\n",
      "FAILED\n",
      "Processing files: 5/140\n",
      "Processing files: 6/140\n",
      "Processing files: 7/140\n",
      "145\n",
      "FAILED\n",
      "Processing files: 8/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/8198688095.dat'\n",
      "145\n",
      "FAILED\n",
      "Processing files: 9/140\n",
      "Processing files: 10/140\n",
      "Processing files: 11/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/5584579577.dat'\n",
      "Processing files: 12/140\n",
      "Processing files: 13/140\n",
      "Processing files: 14/140\n",
      "Processing files: 15/140\n",
      "Processing files: 16/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/4887088159.dat'\n",
      "Processing files: 17/140\n",
      "Processing files: 18/140\n",
      "Processing files: 19/140\n",
      "Processing files: 20/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/8020423365.dat'\n",
      "Processing files: 21/140\n",
      "Processing files: 22/140\n",
      "Processing files: 23/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/8113119897.dat'\n",
      "Processing files: 24/140\n",
      "Processing files: 25/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/7746071670.dat'\n",
      "Processing files: 26/140\n",
      "Processing files: 27/140\n",
      "Processing files: 28/140\n",
      "Processing files: 29/140\n",
      "Processing files: 30/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/6632290971.dat'\n",
      "Processing files: 31/140\n",
      "Processing files: 32/140\n",
      "Processing files: 33/140\n",
      "Processing files: 34/140\n",
      "Processing files: 35/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/3711901174.dat'\n",
      "Processing files: 36/140\n",
      "Processing files: 37/140\n",
      "Processing files: 38/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/8998105351.dat'\n",
      "Processing files: 39/140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.8/site-packages/radiomics/glcm.py:599: RuntimeWarning: invalid value encountered in sqrt\n",
      "  imc2 = (1 - numpy.e ** (-2 * (HXY2 - HXY))) ** 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files: 40/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/4192334630.dat'\n",
      "Processing files: 41/140\n",
      "Processing files: 42/140\n",
      "Processing files: 43/140\n",
      "Processing files: 44/140\n",
      "Processing files: 45/140\n",
      "Processing files: 46/140\n",
      "Processing files: 47/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/9759268825.dat'\n",
      "Processing files: 48/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/7486632691.dat'\n",
      "Processing files: 49/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/5949725868.dat'\n",
      "Processing files: 50/140\n",
      "Processing files: 51/140\n",
      "Processing files: 52/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/7839333026.dat'\n",
      "Processing files: 53/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/3506642060.dat'\n",
      "Processing files: 54/140\n",
      "Processing files: 55/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/5785303732.dat'\n",
      "Processing files: 56/140\n",
      "Processing files: 57/140\n",
      "281\n",
      "FAILED\n",
      "Processing files: 58/140\n",
      "Processing files: 59/140\n",
      "Processing files: 60/140\n",
      "Processing files: 61/140\n",
      "Processing files: 62/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/7435840706.dat'\n",
      "Processing files: 63/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/6304981931.dat'\n",
      "Processing files: 64/140\n",
      "Processing files: 65/140\n",
      "Processing files: 66/140\n",
      "Processing files: 67/140\n",
      "Processing files: 68/140\n",
      "Processing files: 69/140\n",
      "Processing files: 70/140\n",
      "Processing files: 71/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/6622354775.dat'\n",
      "Processing files: 72/140\n",
      "Processing files: 73/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/5407932437.dat'\n",
      "Processing files: 74/140\n",
      "Processing files: 75/140\n",
      "Processing files: 76/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/6348579607.dat'\n",
      "Processing files: 77/140\n",
      "Processing files: 78/140\n",
      "Processing files: 79/140\n",
      "Processing files: 80/140\n",
      "Processing files: 81/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/4637792302.dat'\n",
      "Processing files: 82/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/9929208371.dat'\n",
      "Processing files: 83/140\n",
      "Processing files: 84/140\n",
      "Processing files: 85/140\n",
      "Processing files: 86/140\n",
      "Processing files: 87/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/9747837228.dat'\n",
      "Processing files: 88/140\n",
      "Processing files: 89/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/5419400137.dat'\n",
      "Processing files: 90/140\n",
      "Processing files: 91/140\n",
      "Processing files: 92/140\n",
      "Processing files: 93/140\n",
      "Processing files: 94/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/7751296191.dat'\n",
      "Processing files: 95/140\n",
      "Processing files: 96/140\n",
      "Processing files: 97/140\n",
      "Processing files: 98/140\n",
      "Processing files: 99/140\n",
      "Processing files: 100/140\n",
      "Processing files: 101/140\n",
      "Processing files: 102/140\n",
      "Processing files: 103/140\n",
      "239\n",
      "FAILED\n",
      "Processing files: 104/140\n",
      "Processing files: 105/140\n",
      "30\n",
      "FAILED\n",
      "Processing files: 106/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/7806283824.dat'\n",
      "30\n",
      "FAILED\n",
      "Processing files: 107/140\n",
      "Processing files: 108/140\n",
      "Processing files: 109/140\n",
      "Processing files: 110/140\n",
      "Processing files: 111/140\n",
      "Processing files: 112/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/9355019550.dat'\n",
      "Processing files: 113/140\n",
      "Processing files: 114/140\n",
      "Processing files: 115/140\n",
      "Processing files: 116/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/9834743896.dat'\n",
      "Processing files: 117/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/7618621579.dat'\n",
      "Processing files: 118/140\n",
      "150\n",
      "FAILED\n",
      "Processing files: 119/140\n",
      "Processing files: 120/140\n",
      "Processing files: 121/140\n",
      "Processing files: 122/140\n",
      "Processing files: 123/140\n",
      "Processing files: 124/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/9111010259.dat'\n",
      "Processing files: 125/140\n",
      "Processing files: 126/140\n",
      "Processing files: 127/140\n",
      "Processing files: 128/140\n",
      "Processing files: 129/140\n",
      "61\n",
      "FAILED\n",
      "Processing files: 130/140\n",
      "Processing files: 131/140\n",
      "Processing files: 132/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/8682810696.dat'\n",
      "Processing files: 133/140\n",
      "Processing files: 134/140\n",
      "Processing files: 135/140\n",
      "Processing files: 136/140\n",
      "Processing files: 137/140\n",
      "Processing files: 138/140\n",
      "[Errno 2] No such file or directory: './input/data_set_2/superpixels_labels/9683788079.dat'\n",
      "Processing files: 139/140\n",
      "Processing files: 140/140\n",
      "All files have been processed\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"radiomics\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "rags = process_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rags_train, rags_test = train_test_split(rags, train_size=0.8, random_state=1)\n",
    "\n",
    "subgraphs_train = split_into_subgraphs(rags_train)\n",
    "subgraphs_test = split_into_subgraphs(rags_test)\n",
    "\n",
    "nodes_train, edge_indices_train, edges_train, labels_train =  prepare_data(subgraphs_train)\n",
    "nodes_test, edge_indices_test, edges_test, labels_test =  prepare_data(subgraphs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([35545, None, 12]), TensorShape([35545, None, 1]), TensorShape([35545, None, 2])]\n",
      "[TensorShape([8777, None, 12]), TensorShape([8777, None, 1]), TensorShape([8777, None, 2])]\n",
      "(35545,) (8777,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(labels_train)):\n",
    "    edges_train[i] = np.expand_dims(edges_train[i], axis=-1).astype(np.float32)\n",
    "    \n",
    "for i in range(len(labels_test)):\n",
    "    edges_test[i] = np.expand_dims(edges_test[i], axis=-1).astype(np.float32)\n",
    "    \n",
    "nodes_train = ragged_tensor_from_nested_numpy(nodes_train)\n",
    "edges_train = ragged_tensor_from_nested_numpy(edges_train)\n",
    "edge_indices_train = ragged_tensor_from_nested_numpy(edge_indices_train)\n",
    "\n",
    "nodes_test = ragged_tensor_from_nested_numpy(nodes_test)\n",
    "edges_test = ragged_tensor_from_nested_numpy(edges_test)\n",
    "edge_indices_test = ragged_tensor_from_nested_numpy(edge_indices_test)\n",
    "\n",
    "xtrain = nodes_train, edges_train, edge_indices_train\n",
    "xtest = nodes_test, edges_test, edge_indices_test\n",
    "ytrain = np.array(labels_train)\n",
    "ytest = np.array(labels_test)\n",
    "print([x.shape for x in xtrain])\n",
    "print([x.shape for x in xtest])\n",
    "print(ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:kgcnn: Unknown model kwarg normalize_by_weights with value False\n",
      "INFO:kgcnn: Updated model kwargs:\n",
      "{'depth': 1,\n",
      " 'gcn_args': {'activation': 'relu',\n",
      "              'has_unconnected': True,\n",
      "              'is_sorted': False,\n",
      "              'normalize_by_weights': False,\n",
      "              'pooling_method': 'mean',\n",
      "              'units': 64,\n",
      "              'use_bias': True},\n",
      " 'input_embedding': {'edge': {'input_dim': 10, 'output_dim': 64},\n",
      "                     'node': {'input_dim': 55, 'output_dim': 64}},\n",
      " 'inputs': [{'dtype': 'float32',\n",
      "             'name': 'node_attributes',\n",
      "             'ragged': True,\n",
      "             'shape': (None, 12)},\n",
      "            {'dtype': 'float32',\n",
      "             'name': 'edge_attributes',\n",
      "             'ragged': True,\n",
      "             'shape': (None, 1)},\n",
      "            {'dtype': 'int64',\n",
      "             'name': 'edge_indices',\n",
      "             'ragged': True,\n",
      "             'shape': (None, 2)}],\n",
      " 'name': 'GCN',\n",
      " 'output_embedding': 'graph',\n",
      " 'output_mlp': {'activation': ['relu', 'relu', 'sigmoid'],\n",
      "                'units': [140, 70, 1],\n",
      "                'use_bias': [True, True, False]},\n",
      " 'verbose': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "node_attributes (InputLayer)    [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 64)     832         node_attributes[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "edge_attributes (InputLayer)    [(None, None, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_indices (InputLayer)       [(None, None, 2)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gcn (GCN)                       (None, None, 64)     4160        dense[0][0]                      \n",
      "                                                                 edge_attributes[0][0]            \n",
      "                                                                 edge_indices[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pooling_nodes (PoolingNodes)    (None, 64)           0           gcn[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "mlp (MLP)                       (None, 1)            19040       pooling_nodes[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 24,032\n",
      "Trainable params: 24,032\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/gcn/pooling_weighted_local_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/gcn/pooling_weighted_local_edges/Reshape:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/gcn/pooling_weighted_local_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/gcn/gather_nodes_outgoing/Reshape_1:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/model/gcn/gather_nodes_outgoing/Reshape:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/gcn/gather_nodes_outgoing/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111/1111 - 3s - loss: 0.4616 - accuracy: 0.8344\n",
      "Epoch 2/150\n",
      "1111/1111 - 2s - loss: 0.2982 - accuracy: 0.8715\n",
      "Epoch 3/150\n",
      "1111/1111 - 2s - loss: 0.2694 - accuracy: 0.8834\n",
      "Epoch 4/150\n",
      "1111/1111 - 2s - loss: 0.2668 - accuracy: 0.8816\n",
      "Epoch 5/150\n",
      "1111/1111 - 2s - loss: 0.2551 - accuracy: 0.8886\n",
      "Epoch 6/150\n",
      "1111/1111 - 2s - loss: 0.2486 - accuracy: 0.8899\n",
      "Epoch 7/150\n",
      "1111/1111 - 2s - loss: 0.2458 - accuracy: 0.8900\n",
      "Epoch 8/150\n",
      "1111/1111 - 2s - loss: 0.2419 - accuracy: 0.8922\n",
      "Epoch 9/150\n",
      "1111/1111 - 2s - loss: 0.2435 - accuracy: 0.8909\n",
      "Epoch 10/150\n",
      "1111/1111 - 3s - loss: 0.2390 - accuracy: 0.8937 - val_loss: 0.2179 - val_accuracy: 0.8984\n",
      "Epoch 11/150\n",
      "1111/1111 - 2s - loss: 0.2350 - accuracy: 0.8946\n",
      "Epoch 12/150\n",
      "1111/1111 - 2s - loss: 0.2344 - accuracy: 0.8960\n",
      "Epoch 13/150\n",
      "1111/1111 - 2s - loss: 0.2314 - accuracy: 0.8975\n",
      "Epoch 14/150\n",
      "1111/1111 - 2s - loss: 0.2307 - accuracy: 0.8948\n",
      "Epoch 15/150\n",
      "1111/1111 - 3s - loss: 0.2318 - accuracy: 0.8955\n",
      "Epoch 16/150\n",
      "1111/1111 - 2s - loss: 0.2301 - accuracy: 0.8970\n",
      "Epoch 17/150\n",
      "1111/1111 - 3s - loss: 0.2288 - accuracy: 0.8975\n",
      "Epoch 18/150\n",
      "1111/1111 - 2s - loss: 0.2286 - accuracy: 0.8978\n",
      "Epoch 19/150\n",
      "1111/1111 - 2s - loss: 0.2277 - accuracy: 0.8997\n",
      "Epoch 20/150\n",
      "1111/1111 - 3s - loss: 0.2251 - accuracy: 0.8995 - val_loss: 0.2060 - val_accuracy: 0.9058\n",
      "Epoch 21/150\n",
      "1111/1111 - 2s - loss: 0.2233 - accuracy: 0.9009\n",
      "Epoch 22/150\n"
     ]
    }
   ],
   "source": [
    "model = make_model(\n",
    "    name = \"GCN\",\n",
    "    inputs = [{'shape': (None, 12), 'name': \"node_attributes\", 'dtype': 'float32', 'ragged': True},\n",
    "            {'shape': (None, 1), 'name': \"edge_attributes\", 'dtype': 'float32', 'ragged': True},\n",
    "            {'shape': (None, 2), 'name': \"edge_indices\", 'dtype': 'int64', 'ragged': True}],\n",
    "    input_embedding = {\"node\": {\"input_dim\": 55, \"output_dim\": 64},\n",
    "                       \"edge\": {\"input_dim\": 10, \"output_dim\": 64}},\n",
    "    output_embedding =  'graph',\n",
    "    output_mlp = {\"use_bias\": [True, True, False], \"units\": [140, 70, 1],\n",
    "                \"activation\": ['relu', 'relu', 'sigmoid']},\n",
    "    gcn_args = {\"units\": 64, \"use_bias\": True, \"activation\": 'relu', \"pooling_method\": 'mean', \n",
    "                \"normalize_by_weights\": False},\n",
    "    depth = 1\n",
    ")\n",
    "\n",
    "# Set learning rate and epochs\n",
    "learning_rate_start = 1e-3\n",
    "learning_rate_stop = 1e-4\n",
    "epo = 150\n",
    "epomin = 100\n",
    "epostep = 10\n",
    "\n",
    "# Compile model with optimizer and loss\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate_start)\n",
    "cbks = LinearLearningRateScheduler(learning_rate_start, learning_rate_stop, epomin, epo)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              weighted_metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Start and time training\n",
    "start = time.process_time()\n",
    "hist = model.fit(xtrain, ytrain,\n",
    "                 epochs=epo,\n",
    "                 batch_size=32,\n",
    "                 callbacks=[cbks],\n",
    "                 validation_freq=epostep,\n",
    "                 validation_data=(xtest, ytest),\n",
    "                 verbose=2\n",
    "                 )\n",
    "stop = time.process_time()\n",
    "print(\"Print Time for taining: \", stop - start)\n",
    "\n",
    "# Get loss from history\n",
    "trainlossall = np.array(hist.history['accuracy'])\n",
    "testlossall = np.array(hist.history['val_accuracy'])\n",
    "acc_valid = testlossall[-1]\n",
    "\n",
    "# Plot loss vs epochs\n",
    "plt.figure()\n",
    "plt.plot(np.arange(trainlossall.shape[0]), trainlossall, label='Training ACC', c='blue')\n",
    "plt.plot(np.arange(epostep, epo + epostep, epostep), testlossall, label='Test ACC', c='red')\n",
    "plt.scatter([trainlossall.shape[0]], [acc_valid], label=\"{0:0.4f} \".format(acc_valid), c='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Interaction Network Loss')\n",
    "plt.legend(loc='upper right', fontsize='x-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability = model.predict(xtest)\n",
    "predictions = np.round(probability)\n",
    "comparison = np.concatenate((predictions, ytest, probability), axis=1)\n",
    "# print(comparison)\n",
    "\n",
    "hole = 0\n",
    "true = 0\n",
    "false = 0\n",
    "hole2 = 0\n",
    "true2 = 0\n",
    "false2 = 0\n",
    "for value in comparison:\n",
    "    if value[1] == 1.:\n",
    "        hole += 1\n",
    "        if value[0] == 0.:\n",
    "            false +=1\n",
    "        else:\n",
    "            true +=1\n",
    "    if value[1] == 0.:\n",
    "        hole2 += 1\n",
    "        if value[0] == 1.:\n",
    "            false2 +=1\n",
    "        else:\n",
    "            true2 +=1\n",
    "print(f\"{true}/{hole}\")\n",
    "print(f\"{false}/{hole}\")\n",
    "print(f\"{true2}/{hole2}\")\n",
    "print(f\"{false2}/{hole2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_images = {}\n",
    "\n",
    "for (i, subgraph) in enumerate(subgraphs_test):\n",
    "    if subgraph.rag.file_id not in predicted_images:\n",
    "        predicted_images[subgraph.rag.file_id] = np.copy(subgraph.rag.superpixels_labels)\n",
    "    predicted_images[subgraph.rag.file_id][ predicted_images[subgraph.rag.file_id] == subgraph.superpixel_label] = predictions[i] - 2\n",
    "\n",
    "for key in predicted_images.keys():\n",
    "    predicted_images[key][predicted_images[key] == -1] = 1\n",
    "    predicted_images[key][predicted_images[key] == -2] = 0\n",
    "    predicted_images[key]=predicted_images[key]*255\n",
    "    im = Image.fromarray(predicted_images[key].astype(np.uint8))\n",
    "    im.save(f\"predictions/{key}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
