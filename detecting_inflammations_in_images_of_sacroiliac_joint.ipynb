{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUsage:\\n1. Directory with data sets must be placed in directory named \"input\"\\n2. Input directory must be in the same directory as the .ipynb file with this sript\\n3. Inside each data set directory there must be three directories named: images, labels and masks.\\n4. Filenames of labels and masks files must be the same as image filename.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Usage:\n",
    "1. Directory with data sets must be placed in directory named \"input\"\n",
    "2. Input directory must be in the same directory as the .ipynb file with this sript\n",
    "3. Inside each data set directory there must be three directories named: images, labels and masks.\n",
    "4. Filenames of labels and masks files must be the same as image filename.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "keras-unet init: TF version is >= 2.0.0 - using `tf.keras` instead of `Keras`\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, UpSampling2D, Input, concatenate\n",
    "from keras import backend as K\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.compat.v1 import ConfigProto\n",
    "#from tensorflow.compat.v1 import InteractiveSession\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras_unet.metrics import iou, iou_thresholded\n",
    "from keras_unet.losses import jaccard_distance\n",
    "\n",
    "\n",
    "data_gen_args = dict(\n",
    "        rotation_range=10.,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        shear_range=10,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False,\n",
    "        fill_mode='nearest'\n",
    ")\n",
    "\n",
    "def showOpencvImage(image, isGray=False):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.show()\n",
    "\n",
    "def get_augmented(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    X_val=None,\n",
    "    Y_val=None,\n",
    "    batch_size=32, \n",
    "    seed=0, \n",
    "    data_gen_args = dict(\n",
    "        rotation_range=10.,\n",
    "        width_shift_range=0.02,\n",
    "        height_shift_range=0.02,\n",
    "        zca_whitening = False,\n",
    "        zca_epsilon = 1e-6,\n",
    "        shear_range=5,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False,\n",
    "        fill_mode='nearest'\n",
    "    )):\n",
    "\n",
    "\n",
    "    # Train data, provide the same seed and keyword arguments to the fit and flow methods\n",
    "    X_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    Y_datagen_1 = ImageDataGenerator(**data_gen_args)\n",
    "    Y_datagen_2 = ImageDataGenerator(**data_gen_args)\n",
    "    X_datagen.fit(X_train, augment=True, seed=seed)\n",
    "    Y_train_1 = Y_train[:,:,:,0:1]\n",
    "    Y_train_2 = Y_train[:,:,:,1:2]\n",
    "    Y_datagen_1.fit(Y_train_1, augment=True, seed=seed)\n",
    "    Y_datagen_2.fit(Y_train_2, augment=True, seed=seed)\n",
    "    X_train_augmented = X_datagen.flow(X_train, batch_size=batch_size, shuffle=True, seed=seed)\n",
    "    Y_train_augmented_1 = Y_datagen_1.flow(Y_train_1, batch_size=batch_size, shuffle=True, seed=seed)\n",
    "    Y_train_augmented_2 = Y_datagen_2.flow(Y_train_2, batch_size=batch_size, shuffle=True, seed=seed)\n",
    "    \n",
    "    train_generator = zip(X_train_augmented, Y_train_augmented_1, Y_train_augmented_2)#, Y_train_augmented_3)\n",
    "    return train_generator\n",
    "\n",
    "def my_generator(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    train_gen,\n",
    "    X_val=None,\n",
    "    Y_val=None,\n",
    "    batch_size=2, \n",
    "    seed=0, \n",
    "    data_gen_args = dict(\n",
    "        rotation_range=10.,\n",
    "        width_shift_range=0.02,\n",
    "        height_shift_range=0.02,\n",
    "        zca_whitening = False,\n",
    "        zca_epsilon = 1e-6,\n",
    "        shear_range=5,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False,\n",
    "        fill_mode='nearest'\n",
    "    )):\n",
    "    while 1:\n",
    "        sample_batch = next(train_gen)\n",
    "        xx, yy1,yy2 = sample_batch\n",
    "        yy = np.zeros((xx.shape[0],xx.shape[1],xx.shape[2],2),dtype=np.float32)\n",
    "        yy[:,:,:,0:1] = yy1\n",
    "        yy[:,:,:,1:2] = yy2\n",
    "#        yy[:,:,:,6:7] = yy3\n",
    "        yield (xx, yy)\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def upsample_conv(filters, kernel_size, strides, padding):\n",
    "    return Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\n",
    "\n",
    "def upsample_simple(filters, kernel_size, strides, padding):\n",
    "    return UpSampling2D(strides)\n",
    "\n",
    "def conv2d_block(\n",
    "    inputs, \n",
    "    filters=16, \n",
    "    kernel_size=(3,3), \n",
    "    activation='tanh', \n",
    "#    kernel_initializer='he_normal', \n",
    "    kernel_initializer= 'glorot_uniform',\n",
    "    padding='same'):\n",
    "    \n",
    "    c = Conv2D(filters, kernel_size, activation=activation, kernel_initializer=kernel_initializer, padding=padding) (inputs)\n",
    "    c = Conv2D(filters, kernel_size, activation=activation, kernel_initializer=kernel_initializer, padding=padding) (c)\n",
    "    return c\n",
    "\n",
    "def my_custom_unet(\n",
    "    input_shape,\n",
    "    num_classes=1,\n",
    "    upsample_mode='deconv', # 'deconv' or 'simple' \n",
    "    filters=16,\n",
    "    num_layers=4,\n",
    "    output_activation='softmax'): # 'sigmoid' or 'softmax'\n",
    "    \n",
    "    if upsample_mode=='deconv':\n",
    "        upsample=upsample_conv\n",
    "    else:\n",
    "        upsample=upsample_simple\n",
    "\n",
    "    # Build U-Net model\n",
    "    inputs = Input(input_shape)\n",
    "    x = inputs   \n",
    "\n",
    "    down_layers = []\n",
    "    for l in range(num_layers):\n",
    "        x = conv2d_block(inputs=x, filters=filters)\n",
    "        down_layers.append(x)\n",
    "        x = MaxPooling2D((2, 2)) (x)\n",
    "        filters = filters*2 # double the number of filters with each layer\n",
    "\n",
    "    x = conv2d_block(inputs=x, filters=196)\n",
    "\n",
    "\n",
    "    for conv in reversed(down_layers):        \n",
    "        filters //= 2 # decreasing number of filters with each layer \n",
    "        x = upsample(filters, (2, 2), strides=(2, 2), padding='same') (x)\n",
    "        x = concatenate([x, conv])\n",
    "        x = conv2d_block(inputs=x, filters=filters)\n",
    "    \n",
    "    outputs = Conv2D(num_classes, (1, 1), activation=output_activation) (x)    \n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "def iou(y_true, y_pred, smooth=1.):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "\n",
    "def train_UNET(graphs_train, graphs_test):\n",
    "    images_train = list()\n",
    "    for graph in graphs_train:\n",
    "        images_train.append(f\"{IMAGES_DIR_PATH}/{graph.file_id}.bmp\")\n",
    "\n",
    "    masks = glob.glob(f\"{MASKS_DIR_PATH}/*.bmp\")\n",
    "    orgs = glob.glob(f\"{IMAGES_DIR_PATH}/*.bmp\")\n",
    "\n",
    "    masks.sort()\n",
    "    orgs.sort()\n",
    "\n",
    "    list_train = []\n",
    "    list_test =  []\n",
    "\n",
    "    for (i, org) in enumerate(orgs):\n",
    "        for (j, img_train) in enumerate(images_train):\n",
    "            if org == img_train:\n",
    "                list_train.append(i)\n",
    "                break\n",
    "            elif j == len(images_train)-1:\n",
    "                list_test.append(i)\n",
    "                \n",
    "    print(len(list_train))\n",
    "    print(len(list_test))\n",
    "    print(len(masks))\n",
    "    print(len(orgs))\n",
    "\n",
    "    imgs_list = []\n",
    "    masks_list = []\n",
    "\n",
    "    size = (128,128)\n",
    "\n",
    "    for image, mask in zip(orgs, masks):\n",
    "        im = cv2.imread(image)\n",
    "        im = im[:,:,0]\n",
    "        imgs_list.append(im)\n",
    "\n",
    "        im = cv2.imread(mask)\n",
    "        im = im[:,:,0]\n",
    "\n",
    "        imMask = np.zeros((im.shape[0],im.shape[1],2),dtype=np.float32)\n",
    "        imMask[im == 0,0] = 1               #background\n",
    "        imMask[im!=0,1] = 1                 #spine\n",
    "\n",
    "        masks_list.append(imMask)\n",
    "\n",
    " \n",
    "        \n",
    "    imgs_np = np.asarray(imgs_list)\n",
    "    masks_np = np.asarray(masks_list)\n",
    "    print(imgs_np.shape, masks_np.shape)\n",
    "\n",
    "    weights = np.ones((2),dtype=np.float32)\n",
    "    for i in range(0,2):\n",
    "        weights[i] = 1/(np.sum(masks_np[:,:,:,i])/(masks_np.shape[0]*masks_np.shape[1]*masks_np.shape[2]))\n",
    "\n",
    "    w = sum(weights)\n",
    "    weights = weights/w\n",
    "\n",
    "    print(weights)\n",
    "\n",
    "    print(imgs_np.max(), masks_np.max())\n",
    "    x = np.asarray(imgs_np, dtype=np.float32)\n",
    "    y = np.asarray(masks_np, dtype=np.float32)\n",
    "    print(x.max(), y.max())\n",
    "    print(x.shape, y.shape)\n",
    "    y = y.reshape(y.shape[0], y.shape[1], y.shape[2], 2)\n",
    "    print(x.shape, y.shape)\n",
    "    x = x.reshape(x.shape[0], x.shape[1], x.shape[2], 1)\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "    list_val = list_train[:int(0.2*len(list_train))]\n",
    "    list_train = list_train[int(0.2*len(list_train)):]\n",
    "\n",
    "    print(list_train)\n",
    "    print(list_val)\n",
    "    print(list_test)\n",
    "\n",
    "    x_train = x[list_train]\n",
    "    x_val = x[list_val]\n",
    "    x_test = x[list_test]\n",
    "\n",
    "    y_train = y[list_train]\n",
    "    y_val = y[list_val]\n",
    "    y_test = y[list_test]\n",
    "\n",
    "    print(\"x_train: \", x_train.shape)\n",
    "    print(\"y_train: \", y_train.shape)\n",
    "    print(\"x_val: \", x_val.shape)\n",
    "    print(\"y_val: \", y_val.shape)\n",
    "    print(\"x_test: \", x_test.shape)\n",
    "    print(\"y_test: \", y_test.shape)\n",
    "\n",
    "    input_shape = x_train[0].shape\n",
    "    model = my_custom_unet(\n",
    "            input_shape,\n",
    "            num_classes=2,\n",
    "            filters=64,\n",
    "            output_activation='softmax',\n",
    "            num_layers=4  \n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(f\"saved_objects/unet\"):\n",
    "\n",
    "\n",
    "        model_filename = f\"saved_objects/unet\"\n",
    "\n",
    "        callback_checkpoint = ModelCheckpoint(\n",
    "            model_filename, \n",
    "            verbose=1, \n",
    "            monitor='val_loss', \n",
    "            save_best_only=True\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(lr=0.0001), \n",
    "            loss = weighted_categorical_crossentropy(weights),\n",
    "            metrics=[iou]\n",
    "        )\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        train_gen = get_augmented(x_train, y_train, batch_size=2,data_gen_args=data_gen_args)\n",
    "        generator = my_generator(x_train, y_train,train_gen, batch_size=2,data_gen_args=data_gen_args)\n",
    "\n",
    "        history = model.fit_generator(\n",
    "            generator,\n",
    "            steps_per_epoch=30,\n",
    "            epochs=150,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=[callback_checkpoint]\n",
    "        )\n",
    "\n",
    "\n",
    "    model.load_weights(f\"saved_objects/unet\")\n",
    "    \n",
    "    GENERATED_UNET_DIR_PATH = f\"{OUTPUT_DIR_PATH}/generated_unet\"\n",
    "    if os.path.exists(GENERATED_UNET_DIR_PATH):\n",
    "        shutil.rmtree(GENERATED_UNET_DIR_PATH)\n",
    "    os.makedirs(GENERATED_UNET_DIR_PATH)\n",
    "    \n",
    "    for N in range(x_test.shape[0]):\n",
    "        y_pred = model.predict(x_test[N:N+1])\n",
    "        predictions = np.round(y_pred)\n",
    "        for k in range(1,2):\n",
    "            dum = predictions[0,:,:,k]*255\n",
    "            filename = os.path.basename(orgs[list_test[N]])[:-4]\n",
    "            cv2.imwrite(f\"{GENERATED_UNET_DIR_PATH}/{filename}.png\",dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.future.graph as skigraph\n",
    "import shutil\n",
    "import pickle\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import backend as K\n",
    "\n",
    "from kgcnn.literature.GraphSAGE import make_model\n",
    "from kgcnn.utils.data import ragged_tensor_from_nested_numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import DBSCAN\n",
    "from kgcnn.utils.learning import LinearLearningRateScheduler\n",
    "from PIL import Image, ImageOps\n",
    "import radiomics\n",
    "import numpy.ma as ma\n",
    "import nrrd\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script parameterss\n",
    "DATA_SET_DIR_NAMES = [\n",
    "    #\"250_pixel_15_com\",\n",
    "    #\"250_pixel_25_com\",\n",
    "    #\"250_pixel_35_com\",\n",
    "    \"300_pixel_10_com\",  \n",
    "    #\"300_pixel_20_com\",  \n",
    "    #\"300_pixel_30_com\",  \n",
    "    #\"300_pixel_40_com\",\n",
    "    #\"250_pixel_20_com\",\n",
    "    #\"250_pixel_30_com\", \n",
    "    #\"250_pixel_40_com\",  \n",
    "    #\"300_pixel_15_com\",  \n",
    "    #\"300_pixel_25_com\", \n",
    "    #\"300_pixel_35_com\",  \n",
    "    #\"300_pixel_5_com\"\n",
    "]\n",
    "PYRADIOMICS_FEATURES = [\n",
    "    \"original_firstorder_Mean\",\n",
    "    \"original_firstorder_10Percentile\",\n",
    "    \"original_firstorder_90Percentile\",\n",
    "\n",
    "    \"original_firstorder_Entropy\",\n",
    "    \"original_firstorder_InterquartileRange\",\n",
    "    \"original_firstorder_Kurtosis\",\n",
    "    \"original_firstorder_Maximum\",\n",
    "    \"original_firstorder_MeanAbsoluteDeviation\",\n",
    "    \"original_firstorder_Median\",\n",
    "    \"original_firstorder_Minimum\",\n",
    "    \"original_firstorder_Range\",\n",
    "    \"original_firstorder_RobustMeanAbsoluteDeviation\",\n",
    "    \"original_firstorder_RootMeanSquared\",\n",
    "    \"original_firstorder_Skewness\",\n",
    "    \"original_firstorder_Uniformity\",\n",
    "    \"original_firstorder_Variance\",\n",
    "    \"original_glcm_Autocorrelation\",\n",
    "    \"original_glcm_ClusterProminence\",\n",
    "    \"original_glcm_ClusterShade\",\n",
    "    \"original_glcm_ClusterTendency\",\n",
    "    \"original_glcm_Contrast\",\n",
    "    \"original_glcm_Correlation\",\n",
    "    \"original_glcm_DifferenceAverage\",\n",
    "    \"original_glcm_DifferenceEntropy\",\n",
    "    \"original_glcm_DifferenceVariance\",\n",
    "    \"original_glcm_Id\",\n",
    "    \"original_glcm_Idm\",\n",
    "    \"original_glcm_Idmn\",\n",
    "    \"original_glcm_Idn\",\n",
    "    \"original_glcm_Imc1\",\n",
    "    \"original_glcm_Imc2\",\n",
    "    \"original_glcm_InverseVariance\",\n",
    "    \"original_glcm_JointAverage\",\n",
    "    \"original_glcm_JointEnergy\",\n",
    "    \"original_glcm_JointEntropy\",\n",
    "    \"original_glcm_MCC\",\n",
    "    \"original_glcm_MaximumProbability\",\n",
    "    \"original_glcm_SumAverage\",\n",
    "    \"original_glcm_SumEntropy\",\n",
    "    \"original_glcm_SumSquares\",\n",
    "    \"original_gldm_DependenceEntropy\",\n",
    "    \"original_gldm_DependenceNonUniformity\",\n",
    "    \"original_gldm_DependenceNonUniformityNormalized\",\n",
    "    \"original_gldm_DependenceVariance\",\n",
    "    \"original_gldm_GrayLevelNonUniformity\",\n",
    "    \"original_gldm_GrayLevelVariance\",\n",
    "    \"original_gldm_HighGrayLevelEmphasis\",\n",
    "    \"original_gldm_LargeDependenceEmphasis\",\n",
    "    \"original_gldm_LargeDependenceLowGrayLevelEmphasis\",\n",
    "    \"original_gldm_LowGrayLevelEmphasis\",\n",
    "    \"original_gldm_SmallDependenceHighGrayLevelEmphasis\",\n",
    "    \"original_gldm_SmallDependenceLowGrayLevelEmphasis\",\n",
    "    \"original_glrlm_GrayLevelNonUniformity\",\n",
    "    \"original_glrlm_GrayLevelNonUniformityNormalized\",\n",
    "    \"original_glrlm_GrayLevelVariance\",\n",
    "    \"original_glrlm_HighGrayLevelRunEmphasis\",\n",
    "    \"original_glrlm_LongRunEmphasis\",\n",
    "    \"original_glrlm_LongRunHighGrayLevelEmphasis\",\n",
    "    \"original_glrlm_LongRunLowGrayLevelEmphasis\",\n",
    "    \"original_glrlm_LowGrayLevelRunEmphasis\",\n",
    "    \"original_glrlm_RunLengthNonUniformity\",\n",
    "    \"original_glrlm_RunLengthNonUniformityNormalized\",\n",
    "    \"original_glrlm_RunPercentage\",\n",
    "    \"original_glrlm_RunVariance\",\n",
    "    \"original_glrlm_ShortRunEmphasis\",\n",
    "    \"original_glrlm_ShortRunHighGrayLevelEmphasis\",\n",
    "    \"original_glrlm_ShortRunLowGrayLevelEmphasis\",\n",
    "    \"original_glszm_GrayLevelNonUniformity\",\n",
    "    \"original_glszm_GrayLevelNonUniformityNormalized\",\n",
    "    \"original_glszm_GrayLevelVariance\",\n",
    "    \"original_glszm_HighGrayLevelZoneEmphasis\",\n",
    "    \"original_glszm_LargeAreaEmphasis\",\n",
    "    \"original_glszm_LargeAreaHighGrayLevelEmphasis\",\n",
    "    \"original_glszm_LargeAreaLowGrayLevelEmphasis\",\n",
    "    \"original_glszm_LowGrayLevelZoneEmphasis\",\n",
    "    \"original_glszm_SizeZoneNonUniformity\",\n",
    "    \"original_glszm_SizeZoneNonUniformityNormalized\",\n",
    "    \"original_glszm_SmallAreaEmphasis\",\n",
    "    \"original_glszm_SmallAreaLowGrayLevelEmphasis\",\n",
    "    \"original_glszm_ZoneEntropy\",\n",
    "    \"original_glszm_ZonePercentage\",\n",
    "    \"original_glszm_ZoneVariance\",\n",
    "    \"original_ngtdm_Busyness\",\n",
    "    \"original_ngtdm_Coarseness\",\n",
    "    \"original_ngtdm_Complexity\",\n",
    "    \"original_ngtdm_Contrast\",\n",
    "    \"original_ngtdm_Strength\",\n",
    "]\n",
    "\n",
    "WHITE_SUPERPIXEL_LABEL_TRESHOLD = 0.65\n",
    "\n",
    "#configuration\n",
    "logger = logging.getLogger(\"radiomics\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def save_object(filename, obj):\n",
    "    obj_file = open(filename, \"wb\")\n",
    "    pickle.dump(obj, obj_file)\n",
    "    obj_file.close()\n",
    "    \n",
    "def load_object(filename):\n",
    "    obj_file = open(filename, \"rb\")\n",
    "    obj = pickle.load(obj_file)\n",
    "    obj_file.close()\n",
    "    \n",
    "    return obj\n",
    "\n",
    "# semantic segmentation using subgraphs of image graphs\n",
    "class Graph:\n",
    "    def __init__(self, file_id, rag, image, superpixels_labels, mask):\n",
    "        self.file_id = file_id\n",
    "        self.rag = rag\n",
    "        self.image = image\n",
    "        self.superpixels_labels = superpixels_labels\n",
    "        self.mask = mask\n",
    "\n",
    "class Subgraph:\n",
    "    def __init__(self, rag, graph, middle_superpixel_label, label):\n",
    "        self.rag = rag\n",
    "        self.graph = graph\n",
    "        self.middle_superpixel_label = middle_superpixel_label\n",
    "        self.label = label\n",
    "        self.unnormalized_edge_indices = None\n",
    "        self.normalized_edge_indices = None\n",
    "        self.edges = None\n",
    "        self.nodes = None\n",
    "        \n",
    "def assign_label(graph):\n",
    "    white_pixels_count = defaultdict(int)\n",
    "    total_pixels_count = defaultdict(int)\n",
    "    \n",
    "    for (i, row) in enumerate(graph.superpixels_labels):\n",
    "        for (j, superpixel_label) in enumerate(row):\n",
    "            total_pixels_count[superpixel_label] += 1\n",
    "            if graph.mask[i][j] == 1:\n",
    "                white_pixels_count[superpixel_label] += 1\n",
    "      \n",
    "    for node in graph.rag:\n",
    "        graph.rag.nodes[node]['label'] = 1.0 if white_pixels_count[node] / total_pixels_count[node] >= WHITE_SUPERPIXEL_LABEL_TRESHOLD else 0.0\n",
    "            \n",
    "            \n",
    "def assign_features(graph):\n",
    "    unique_superpixels_labels = np.unique(graph.superpixels_labels)\n",
    "    \n",
    "    for superpixel_label in unique_superpixels_labels:\n",
    "        superpixel_label_mask = (graph.superpixels_labels == superpixel_label).astype(int)\n",
    "        NRRD_DIRECTORY_PATH = f\"{OUTPUT_DIR_PATH}/nrrd\"\n",
    "        NRRD_ITEM_DIRECTORY_PATH = f\"{NRRD_DIRECTORY_PATH}/{graph.file_id}\"\n",
    "        os.makedirs(NRRD_ITEM_DIRECTORY_PATH, exist_ok=True)\n",
    "        \n",
    "        os.makedirs(f\"{NRRD_ITEM_DIRECTORY_PATH}\", exist_ok=True)\n",
    "        nrrd.write(f\"{NRRD_ITEM_DIRECTORY_PATH}/{superpixel_label}_image.nrrd\", graph.image)\n",
    "        nrrd.write(f\"{NRRD_ITEM_DIRECTORY_PATH}/{superpixel_label}_superpixel_label_mask.nrrd\", superpixel_label_mask)\n",
    "        nrrd_image_path = os.path.join(f\"{NRRD_ITEM_DIRECTORY_PATH}\", str(superpixel_label) + \"_image.nrrd\")\n",
    "        nrrd_superpixel_label_mask_path = os.path.join(f\"{NRRD_ITEM_DIRECTORY_PATH}\", str(superpixel_label) + \"_superpixel_label_mask.nrrd\")\n",
    "        \n",
    "        extractor = radiomics.featureextractor.RadiomicsFeatureExtractor(minimumROIDimensions=1)\n",
    "        result = extractor.execute(nrrd_image_path, nrrd_superpixel_label_mask_path) \n",
    "\n",
    "        for feature in PYRADIOMICS_FEATURES:\n",
    "            graph.rag.nodes[superpixel_label][feature] = result[feature]\n",
    "    \n",
    "def process_images():\n",
    "    EXPECTED_DIR_PATH = f\"{OUTPUT_DIR_PATH}/expected\"\n",
    "    if os.path.exists(EXPECTED_DIR_PATH):\n",
    "        shutil.rmtree(EXPECTED_DIR_PATH)\n",
    "    os.makedirs(EXPECTED_DIR_PATH)\n",
    "    \n",
    "    filenames = os.listdir(IMAGES_DIR_PATH)\n",
    "    graphs = list()\n",
    "        \n",
    "    for (file_count, filename) in enumerate(filenames, start=1):\n",
    "        print(f\"Processing files: {file_count}/{len(filenames)}\")\n",
    "        \n",
    "        file_id = os.path.splitext(filename)[0]\n",
    "        \n",
    "        try:\n",
    "            image = np.array(ImageOps.grayscale(Image.open(f\"{IMAGES_DIR_PATH}/{filename}\")))\n",
    "            mask = np.array(ImageOps.grayscale(Image.open(f\"{MASKS_DIR_PATH}/{file_id}.bmp\")))\n",
    "            superpixels_labels = np.fromfile(f\"{SUPERPIXELS_LABELS_DIR_PATH}/{file_id}.dat\", dtype=np.dtype((np.int32, image.shape)))[0]\n",
    "        except FileNotFoundError as error: \n",
    "            print(error)\n",
    "            \n",
    "        rag = skigraph.rag_mean_color(image, superpixels_labels)\n",
    "        graphs.append(Graph(file_id, rag, image, superpixels_labels, mask))\n",
    "        assign_label(graphs[-1])\n",
    "        assign_features(graphs[-1])\n",
    "        \n",
    "        expected = Image.fromarray((mask*255).astype(np.uint8))\n",
    "        expected.save(f\"{EXPECTED_DIR_PATH}/{file_id}.png\")\n",
    "        \n",
    "    print(\"All files have been processed\")\n",
    "    \n",
    "    return graphs\n",
    "\n",
    "def split_into_subgraphs(graphs):\n",
    "    subgraphs = []\n",
    "    for graph in graphs:\n",
    "        for node in graph.rag.nodes:\n",
    "            nodes = [neighbor for neighbor in graph.rag.neighbors(node)] + [node]\n",
    "            rag = graph.rag.subgraph(nodes)\n",
    "            label = graph.rag.nodes[node]['label']\n",
    "            subgraphs.append(Subgraph(rag, graph, node, label))\n",
    "            \n",
    "    return subgraphs\n",
    "\n",
    "def normalize_edge_indices(edge_indices):\n",
    "    flat_list = [node for edge_index in edge_indices for node in edge_index]            \n",
    "    flat_list.sort()\n",
    "    flat_list = list(dict.fromkeys(flat_list))\n",
    "    change = {key:value for (value, key) in enumerate(flat_list)}\n",
    "    result = [[change[edge_index[0]], change[edge_index[1]]] for edge_index in edge_indices] \n",
    "    \n",
    "    return result\n",
    "\n",
    "def prepare_data(subgraphs):\n",
    "    nodes = []\n",
    "    edge_indices = []\n",
    "    edges = []\n",
    "    labels = []\n",
    "    \n",
    "    for subgraph in subgraphs:\n",
    "        node_features = []\n",
    "        \n",
    "        for node in subgraph.rag.nodes:\n",
    "            node_features.append([subgraph.rag.nodes[node][feature] for feature in PYRADIOMICS_FEATURES])\n",
    "            \n",
    "        nodes.append(node_features)\n",
    "        edges.append([[1.0] for edge in subgraph.rag.edges.data()])\n",
    "        unnormalized_edge_indices = [list(index) for index in subgraph.rag.edges]\n",
    "        edge_indices.append(normalize_edge_indices(unnormalized_edge_indices))\n",
    "        labels.append(subgraph.label)\n",
    "        \n",
    "        subgraph.nodes = nodes[-1]\n",
    "        subgraph.unnormalized_edge_indices = unnormalized_edge_indices\n",
    "        subgraph.normalized_edge_indices = edge_indices[-1]\n",
    "        subgraph.edges = edges[-1]\n",
    "\n",
    "    return nodes, edge_indices, edges, np.array(labels)\n",
    "\n",
    "def generate_segmented_images_from_predictions(subgraphs, predictions):\n",
    "    predicted_images = {}\n",
    "    GENERATED_DIR_PATH = f\"{OUTPUT_DIR_PATH}/generated\"\n",
    "    if os.path.exists(GENERATED_DIR_PATH):\n",
    "        shutil.rmtree(GENERATED_DIR_PATH)\n",
    "    os.makedirs(GENERATED_DIR_PATH)\n",
    "\n",
    "    for (prediction_count, subgraph) in enumerate(subgraphs):\n",
    "        if subgraph.graph.file_id not in predicted_images:\n",
    "            predicted_images[subgraph.graph.file_id] = np.copy(subgraph.graph.superpixels_labels)\n",
    "        predicted_images[subgraph.graph.file_id][predicted_images[subgraph.graph.file_id] == subgraph.middle_superpixel_label] = predictions[prediction_count] - 2\n",
    "\n",
    "    for file_id in predicted_images.keys():\n",
    "        predicted_images[file_id][predicted_images[file_id] == -1] = 255\n",
    "        predicted_images[file_id][predicted_images[file_id] == -2] = 0\n",
    "        generated = Image.fromarray(predicted_images[file_id].astype(np.uint8))\n",
    "        generated.save(f\"{GENERATED_DIR_PATH}/{file_id}.png\")\n",
    "\n",
    "def compute_dice_coefficient(expected, generated):\n",
    "    expected[expected!=0] = 1\n",
    "    generated[generated!=0] = 1\n",
    "    \n",
    "    return 2*np.sum(expected*generated)/(np.sum(expected)+np.sum(generated))\n",
    "\n",
    "def compute_dice_coefficients_for_test_set(graphs_test, info_file):\n",
    "    info_file.write(f\"\\t\\tGCN\\t\\t\\tUNET\\n\")\n",
    "    for graph in graphs_test:\n",
    "        expected = np.array(ImageOps.grayscale(Image.open(f\"{OUTPUT_DIR_PATH}/expected/{graph.file_id}.png\")))\n",
    "        generated = np.array(ImageOps.grayscale(Image.open(f\"{OUTPUT_DIR_PATH}/generated/{graph.file_id}.png\")))\n",
    "        generated_unet = np.array(ImageOps.grayscale(Image.open(f\"{OUTPUT_DIR_PATH}/generated_unet/{graph.file_id}.png\")))\n",
    "        \n",
    "        dice_coefficient = compute_dice_coefficient(expected, generated)\n",
    "        dice_coefficient_unet = compute_dice_coefficient(expected, generated_unet)\n",
    "        \n",
    "        info_file.write(f\"{graph.file_id}\\t{str(dice_coefficient)}\\t{str(dice_coefficient_unet)}\\n\")\n",
    "  \n",
    "def train_GCN(graphs, info_file, graphs_train, graphs_test, use_saved_model):\n",
    "    subgraphs_train = split_into_subgraphs(graphs_train)\n",
    "    subgraphs_test = split_into_subgraphs(graphs_test)\n",
    "\n",
    "    nodes_train, edge_indices_train, edges_train, labels_train =  prepare_data(subgraphs_train)\n",
    "    nodes_test, edge_indices_test, edges_test, labels_test =  prepare_data(subgraphs_test)\n",
    "\n",
    "    nodes_train = ragged_tensor_from_nested_numpy(nodes_train)\n",
    "    edges_train = ragged_tensor_from_nested_numpy(edges_train)\n",
    "    edge_indices_train = ragged_tensor_from_nested_numpy(edge_indices_train)\n",
    "\n",
    "    nodes_test = ragged_tensor_from_nested_numpy(nodes_test)\n",
    "    edges_test = ragged_tensor_from_nested_numpy(edges_test)\n",
    "    edge_indices_test = ragged_tensor_from_nested_numpy(edge_indices_test)\n",
    "\n",
    "    xtrain = nodes_train, edges_train, edge_indices_train\n",
    "    xtest = nodes_test, edges_test, edge_indices_test\n",
    "    ytrain = labels_train\n",
    "    ytest = labels_test\n",
    "    print([x.shape for x in xtrain])\n",
    "    print([x.shape for x in xtest])\n",
    "    print(ytrain.shape, ytest.shape)\n",
    "    info_file.write(f\"nodes_train: {xtrain[0].shape}\\n\")\n",
    "    info_file.write(f\"edges_train: {xtrain[1].shape}\\n\")\n",
    "    info_file.write(f\"edge_indices_train: {xtrain[2].shape}\\n\")\n",
    "    info_file.write(f\"labels_train: {ytrain.shape}\\n\")\n",
    "    info_file.write(f\"nodes_test: {xtest[0].shape}\\n\")\n",
    "    info_file.write(f\"edges_test: {xtest[1].shape}\\n\")\n",
    "    info_file.write(f\"edge_indices_test: {xtest[2].shape}\\n\")\n",
    "    info_file.write(f\"labels_test: {ytest.shape}\\n\")\n",
    "    \n",
    "    MODEL_FILE_PATH = f\"{SAVED_OBJECTS_DIR_PATH}/model\"\n",
    "    if os.path.exists(MODEL_FILE_PATH) and use_saved_model:\n",
    "        model = keras.models.load_model(MODEL_FILE_PATH)\n",
    "    else:\n",
    "        model = make_model(\n",
    "            name= \"GraphSAGE\",\n",
    "            inputs= [{'shape': (None,len(PYRADIOMICS_FEATURES)), 'name': \"node_attributes\", 'dtype': 'float32', 'ragged': True},\n",
    "                    {'shape': (None,), 'name': \"edge_attributes\", 'dtype': 'float32', 'ragged': True},\n",
    "                    {'shape': (None, 2), 'name': \"edge_indices\", 'dtype': 'int64', 'ragged': True}],\n",
    "            input_embedding= {\"node\": {\"input_dim\": 95, \"output_dim\": 64},\n",
    "                              \"edge\": {\"input_dim\": 5, \"output_dim\": 64}},\n",
    "            output_embedding= 'graph',\n",
    "            output_mlp= {\"use_bias\": [True, True, False], \"units\": [25, 10, 1],\n",
    "                         \"activation\": ['relu', 'relu', 'sigmoid']},\n",
    "            node_mlp_args= {\"units\": [100, 50], \"use_bias\": True, \"activation\": ['relu', \"linear\"]},\n",
    "            pooling_args= {'pooling_method': \"segment_mean\"}, gather_args= {},\n",
    "            concat_args= {\"axis\": -1},\n",
    "            use_edge_features= False,\n",
    "            pooling_nodes_args= {'pooling_method': \"mean\"},\n",
    "            depth= 1, \n",
    "            verbose= 1\n",
    "        )\n",
    "\n",
    "        learning_rate_start = 1e-3\n",
    "        learning_rate_stop = 1e-4\n",
    "        epo = 50\n",
    "        epomin = 100\n",
    "        epostep = 10\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(lr=learning_rate_start)\n",
    "        cbks = LinearLearningRateScheduler(learning_rate_start, learning_rate_stop, epomin, epo)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      weighted_metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "\n",
    "        start = time.process_time()\n",
    "        hist = model.fit(xtrain, ytrain,\n",
    "                         epochs=epo,\n",
    "                         batch_size=32,\n",
    "                         callbacks=[cbks],\n",
    "                         validation_freq=epostep,\n",
    "                         validation_data=(xtest, ytest),\n",
    "                         verbose=2\n",
    "                         )\n",
    "        stop = time.process_time()\n",
    "        print(\"Print Time for taining: \", stop - start)\n",
    "\n",
    "        trainlossall = np.array(hist.history['accuracy'])\n",
    "        testlossall = np.array(hist.history['val_accuracy'])\n",
    "        acc_valid = testlossall[-1]\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(trainlossall.shape[0]), trainlossall, label='Training ACC', c='blue')\n",
    "        plt.plot(np.arange(epostep, epo + epostep, epostep), testlossall, label='Test ACC', c='red')\n",
    "        plt.scatter([trainlossall.shape[0]], [acc_valid], label=\"{0:0.4f} \".format(acc_valid), c='red')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Interaction Network Loss')\n",
    "        plt.legend(loc='upper right', fontsize='x-large')\n",
    "        \n",
    "        TRAINING_PLOT_FILE_PATH = f\"{OUTPUT_DIR_PATH}/training_plot.png\"\n",
    "        if os.path.exists(TRAINING_PLOT_FILE_PATH):\n",
    "            os.remove(TRAINING_PLOT_FILE_PATH)\n",
    "        plt.savefig(TRAINING_PLOT_FILE_PATH)\n",
    "        plt.show()\n",
    "\n",
    "        model.save(MODEL_FILE_PATH)\n",
    "    \n",
    "    probability = model.predict(xtest)\n",
    "    predictions = np.round(probability)\n",
    "    print(confusion_matrix(ytest, predictions))\n",
    "    info_file.write(np.array2string(confusion_matrix(ytest, predictions))+\"\\n\")\n",
    "    generate_segmented_images_from_predictions(subgraphs_test, predictions)\n",
    "\n",
    "def initialize_dir_paths(data_set_dir_name):\n",
    "    # globals\n",
    "    global DATA_DIR_PATH \n",
    "    global IMAGES_DIR_PATH\n",
    "    global SUPERPIXELS_LABELS_DIR_PATH\n",
    "    global MASKS_DIR_PATH\n",
    "    global OUTPUT_DIR_PATH\n",
    "    global SAVED_OBJECTS_DIR_PATH\n",
    "    \n",
    "    DATA_DIR_PATH = f\"./input/{data_set_dir_name}\"\n",
    "    IMAGES_DIR_PATH = f\"{DATA_DIR_PATH}/images\"\n",
    "    SUPERPIXELS_LABELS_DIR_PATH = f\"{DATA_DIR_PATH}/superpixels_labels\"\n",
    "    MASKS_DIR_PATH = f\"{DATA_DIR_PATH}/masks\"\n",
    "    \n",
    "    OUTPUT_DIR_PATH = f\"./output/{data_set_dir_name}\"\n",
    "    SAVED_OBJECTS_DIR_PATH = f\"./saved_objects/{data_set_dir_name}\"\n",
    "    \n",
    "def process_data_set(data_set_dir_name, use_saved_graphs=True, use_saved_model=True):  \n",
    "    initialize_dir_paths(data_set_dir_name)\n",
    "    os.makedirs(f\"{OUTPUT_DIR_PATH}\", exist_ok=True)\n",
    "    os.makedirs(f\"{SAVED_OBJECTS_DIR_PATH}\", exist_ok=True)\n",
    "    \n",
    "    GRAPHS_FILE_PATH = f\"{SAVED_OBJECTS_DIR_PATH}/graphs\"\n",
    "    if os.path.exists(GRAPHS_FILE_PATH) and use_saved_graphs:\n",
    "        graphs = load_object(GRAPHS_FILE_PATH)\n",
    "    else:\n",
    "        graphs = process_images()\n",
    "        save_object(GRAPHS_FILE_PATH, graphs)\n",
    "\n",
    "    graphs_train, graphs_test = train_test_split(graphs, train_size=0.8, random_state=1)\n",
    "\n",
    "    INFO_FILE_PATH = f\"{OUTPUT_DIR_PATH}/info.txt\"\n",
    "    if os.path.exists(INFO_FILE_PATH):\n",
    "        os.remove(INFO_FILE_PATH)\n",
    "    with open(INFO_FILE_PATH,\"w+\") as info_file:\n",
    "        info_file.write(f\"data set directory name: {data_set_dir_name}\\n\")\n",
    "        \n",
    "        train_GCN(graphs, info_file, graphs_train, graphs_test, use_saved_model)\n",
    "        train_UNET(graphs_train, graphs_test)\n",
    "        compute_dice_coefficients_for_test_set(graphs_test, info_file)\n",
    "    \n",
    "def process_data_sets(use_saved_graphs=True, use_saved_model=True):\n",
    "    for data_set_dir_name in DATA_SET_DIR_NAMES:\n",
    "        process_data_set(data_set_dir_name, use_saved_graphs, use_saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nname = \"GCN\",\\ninputs = [{\\'shape\\': (None, len(PYRADIOMICS_FEATURES)), \\'name\\': \"node_attributes\", \\'dtype\\': \\'float32\\', \\'ragged\\': True},\\n        {\\'shape\\': (None, 1), \\'name\\': \"edge_attributes\", \\'dtype\\': \\'float32\\', \\'ragged\\': True},\\n        {\\'shape\\': (None, 2), \\'name\\': \"edge_indices\", \\'dtype\\': \\'int64\\', \\'ragged\\': True}],\\ninput_embedding = {\"node\": {\"input_dim\": 300, \"output_dim\": 400},\\n                   \"edge\": {\"input_dim\": 10, \"output_dim\": 64}},\\noutput_embedding =  \\'graph\\',\\noutput_mlp = {\"use_bias\": [True, True, False], \"units\": [140, 70, 1],\\n            \"activation\": [\\'relu\\', \\'relu\\', \\'sigmoid\\']},\\ngcn_args = {\"units\": 64, \"use_bias\": False, \"activation\": \\'relu\\', \"pooling_method\": \\'sum\\', \\n            \"normalize_by_weights\": False},\\ndepth = 1\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            '''\n",
    "            name = \"GCN\",\n",
    "            inputs = [{'shape': (None, len(PYRADIOMICS_FEATURES)), 'name': \"node_attributes\", 'dtype': 'float32', 'ragged': True},\n",
    "                    {'shape': (None, 1), 'name': \"edge_attributes\", 'dtype': 'float32', 'ragged': True},\n",
    "                    {'shape': (None, 2), 'name': \"edge_indices\", 'dtype': 'int64', 'ragged': True}],\n",
    "            input_embedding = {\"node\": {\"input_dim\": 300, \"output_dim\": 400},\n",
    "                               \"edge\": {\"input_dim\": 10, \"output_dim\": 64}},\n",
    "            output_embedding =  'graph',\n",
    "            output_mlp = {\"use_bias\": [True, True, False], \"units\": [140, 70, 1],\n",
    "                        \"activation\": ['relu', 'relu', 'sigmoid']},\n",
    "            gcn_args = {\"units\": 64, \"use_bias\": False, \"activation\": 'relu', \"pooling_method\": 'sum', \n",
    "                        \"normalize_by_weights\": False},\n",
    "            depth = 1\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([28055, None, 87]), TensorShape([28055, None, 1]), TensorShape([28055, None, 2])]\n",
      "[TensorShape([7583, None, 87]), TensorShape([7583, None, 1]), TensorShape([7583, None, 2])]\n",
      "(28055,) (7583,)\n",
      "INFO:kgcnn: Updated model kwargs:\n",
      "{'concat_args': {'axis': -1},\n",
      " 'depth': 1,\n",
      " 'edge_mlp_args': {'activation': ['relu', 'linear'],\n",
      "                   'units': [100, 50],\n",
      "                   'use_bias': True},\n",
      " 'gather_args': {},\n",
      " 'input_embedding': {'edge': {'input_dim': 5, 'output_dim': 64},\n",
      "                     'node': {'input_dim': 95, 'output_dim': 64}},\n",
      " 'inputs': [{'dtype': 'float32',\n",
      "             'name': 'node_attributes',\n",
      "             'ragged': True,\n",
      "             'shape': (None, 87)},\n",
      "            {'dtype': 'float32',\n",
      "             'name': 'edge_attributes',\n",
      "             'ragged': True,\n",
      "             'shape': (None,)},\n",
      "            {'dtype': 'int64',\n",
      "             'name': 'edge_indices',\n",
      "             'ragged': True,\n",
      "             'shape': (None, 2)}],\n",
      " 'name': 'GraphSAGE',\n",
      " 'node_mlp_args': {'activation': ['relu', 'linear'],\n",
      "                   'units': [100, 50],\n",
      "                   'use_bias': True},\n",
      " 'output_embedding': 'graph',\n",
      " 'output_mlp': {'activation': ['relu', 'relu', 'sigmoid'],\n",
      "                'units': [25, 10, 1],\n",
      "                'use_bias': [True, True, False]},\n",
      " 'pooling_args': {'pooling_method': 'segment_mean'},\n",
      " 'pooling_nodes_args': {'pooling_method': 'mean'},\n",
      " 'use_edge_features': False,\n",
      " 'verbose': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "node_attributes (InputLayer)    [(None, None, 87)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_indices (InputLayer)       [(None, None, 2)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gather_nodes_outgoing (GatherNo (None, None, 87)     0           node_attributes[0][0]            \n",
      "                                                                 edge_indices[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mlp (MLP)                       (None, None, 50)     13850       gather_nodes_outgoing[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pooling_local_edges (PoolingLoc (None, None, 50)     0           node_attributes[0][0]            \n",
      "                                                                 mlp[0][0]                        \n",
      "                                                                 edge_indices[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 137)    0           node_attributes[0][0]            \n",
      "                                                                 pooling_local_edges[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "mlp_1 (MLP)                     (None, None, 50)     18850       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, None, 50)     100         mlp_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pooling_nodes (PoolingNodes)    (None, 50)           0           layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "mlp_2 (MLP)                     (None, 1)            1545        pooling_nodes[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "edge_attributes (InputLayer)    [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1)            0           mlp_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 34,345\n",
      "Trainable params: 34,345\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/pooling_local_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/pooling_local_edges/Reshape:0\", shape=(None, 50), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/pooling_local_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "877/877 - 4s - loss: 0.2654 - accuracy: 0.8841\n",
      "Epoch 2/50\n",
      "877/877 - 3s - loss: 0.2077 - accuracy: 0.9125\n",
      "Epoch 3/50\n",
      "877/877 - 3s - loss: 0.1910 - accuracy: 0.9204\n",
      "Epoch 4/50\n",
      "877/877 - 3s - loss: 0.1793 - accuracy: 0.9282\n",
      "Epoch 5/50\n",
      "877/877 - 3s - loss: 0.1707 - accuracy: 0.9290\n",
      "Epoch 6/50\n",
      "877/877 - 3s - loss: 0.1663 - accuracy: 0.9311\n",
      "Epoch 7/50\n",
      "877/877 - 3s - loss: 0.1628 - accuracy: 0.9331\n",
      "Epoch 8/50\n",
      "877/877 - 3s - loss: 0.1627 - accuracy: 0.9327\n",
      "Epoch 9/50\n",
      "877/877 - 3s - loss: 0.1543 - accuracy: 0.9362\n",
      "Epoch 10/50\n",
      "877/877 - 4s - loss: 0.1538 - accuracy: 0.9359 - val_loss: 0.1350 - val_accuracy: 0.9457\n",
      "Epoch 11/50\n",
      "877/877 - 3s - loss: 0.1515 - accuracy: 0.9370\n",
      "Epoch 12/50\n",
      "877/877 - 3s - loss: 0.1484 - accuracy: 0.9377\n",
      "Epoch 13/50\n",
      "877/877 - 3s - loss: 0.1476 - accuracy: 0.9402\n",
      "Epoch 14/50\n",
      "877/877 - 3s - loss: 0.1467 - accuracy: 0.9380\n",
      "Epoch 15/50\n",
      "877/877 - 3s - loss: 0.1428 - accuracy: 0.9415\n",
      "Epoch 16/50\n",
      "877/877 - 3s - loss: 0.1415 - accuracy: 0.9421\n",
      "Epoch 17/50\n",
      "877/877 - 3s - loss: 0.1410 - accuracy: 0.9422\n",
      "Epoch 18/50\n",
      "877/877 - 3s - loss: 0.1383 - accuracy: 0.9425\n",
      "Epoch 19/50\n",
      "877/877 - 3s - loss: 0.1377 - accuracy: 0.9430\n",
      "Epoch 20/50\n",
      "877/877 - 3s - loss: 0.1362 - accuracy: 0.9420 - val_loss: 0.1425 - val_accuracy: 0.9421\n",
      "Epoch 21/50\n",
      "877/877 - 3s - loss: 0.1351 - accuracy: 0.9440\n",
      "Epoch 22/50\n",
      "877/877 - 3s - loss: 0.1352 - accuracy: 0.9440\n",
      "Epoch 23/50\n",
      "877/877 - 3s - loss: 0.1324 - accuracy: 0.9443\n",
      "Epoch 24/50\n",
      "877/877 - 3s - loss: 0.1312 - accuracy: 0.9448\n",
      "Epoch 25/50\n",
      "877/877 - 3s - loss: 0.1284 - accuracy: 0.9474\n",
      "Epoch 26/50\n",
      "877/877 - 3s - loss: 0.1294 - accuracy: 0.9464\n",
      "Epoch 27/50\n",
      "877/877 - 3s - loss: 0.1282 - accuracy: 0.9458\n",
      "Epoch 28/50\n",
      "877/877 - 3s - loss: 0.1258 - accuracy: 0.9463\n",
      "Epoch 29/50\n",
      "877/877 - 3s - loss: 0.1249 - accuracy: 0.9476\n",
      "Epoch 30/50\n",
      "877/877 - 3s - loss: 0.1245 - accuracy: 0.9471 - val_loss: 0.1247 - val_accuracy: 0.9495\n",
      "Epoch 31/50\n",
      "877/877 - 3s - loss: 0.1220 - accuracy: 0.9498\n",
      "Epoch 32/50\n",
      "877/877 - 3s - loss: 0.1218 - accuracy: 0.9489\n",
      "Epoch 33/50\n",
      "877/877 - 3s - loss: 0.1204 - accuracy: 0.9486\n",
      "Epoch 34/50\n",
      "877/877 - 3s - loss: 0.1214 - accuracy: 0.9499\n",
      "Epoch 35/50\n",
      "877/877 - 3s - loss: 0.1190 - accuracy: 0.9514\n",
      "Epoch 36/50\n",
      "877/877 - 3s - loss: 0.1181 - accuracy: 0.9502\n",
      "Epoch 37/50\n",
      "877/877 - 3s - loss: 0.1186 - accuracy: 0.9505\n",
      "Epoch 38/50\n",
      "877/877 - 3s - loss: 0.1154 - accuracy: 0.9505\n",
      "Epoch 39/50\n",
      "877/877 - 3s - loss: 0.1165 - accuracy: 0.9522\n",
      "Epoch 40/50\n",
      "877/877 - 3s - loss: 0.1132 - accuracy: 0.9526 - val_loss: 0.1336 - val_accuracy: 0.9466\n",
      "Epoch 41/50\n",
      "877/877 - 3s - loss: 0.1155 - accuracy: 0.9515\n",
      "Epoch 42/50\n",
      "877/877 - 3s - loss: 0.1120 - accuracy: 0.9533\n",
      "Epoch 43/50\n",
      "877/877 - 3s - loss: 0.1113 - accuracy: 0.9525\n",
      "Epoch 44/50\n",
      "877/877 - 3s - loss: 0.1118 - accuracy: 0.9542\n",
      "Epoch 45/50\n",
      "877/877 - 3s - loss: 0.1106 - accuracy: 0.9538\n",
      "Epoch 46/50\n",
      "877/877 - 3s - loss: 0.1083 - accuracy: 0.9541\n",
      "Epoch 47/50\n",
      "877/877 - 3s - loss: 0.1103 - accuracy: 0.9534\n",
      "Epoch 48/50\n",
      "877/877 - 3s - loss: 0.1064 - accuracy: 0.9553\n",
      "Epoch 49/50\n",
      "877/877 - 3s - loss: 0.1062 - accuracy: 0.9558\n",
      "Epoch 50/50\n",
      "877/877 - 3s - loss: 0.1068 - accuracy: 0.9553 - val_loss: 0.1341 - val_accuracy: 0.9474\n",
      "Print Time for taining:  395.411481654\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_5 in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as concatenate_1_layer_call_fn, concatenate_1_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn, layer_normalization_1_layer_call_and_return_conditional_losses, mlp_dense_0_layer_call_fn while saving (showing 5 of 80). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_objects/300_pixel_10_com/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_objects/300_pixel_10_com/model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5729  179]\n",
      " [ 220 1455]]\n",
      "112\n",
      "28\n",
      "140\n",
      "140\n",
      "(140, 128, 128) (140, 128, 128, 2)\n",
      "[0.22539543 0.77460456]\n",
      "255 1.0\n",
      "255.0 1.0\n",
      "(140, 128, 128) (140, 128, 128, 2)\n",
      "(140, 128, 128) (140, 128, 128, 2)\n",
      "(140, 128, 128, 1) (140, 128, 128, 2)\n",
      "[28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 42, 44, 45, 46, 47, 49, 50, 51, 52, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 73, 75, 77, 78, 79, 82, 83, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 104, 105, 106, 108, 109, 110, 111, 112, 114, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139]\n",
      "[0, 2, 3, 4, 6, 7, 10, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "[1, 5, 8, 9, 17, 18, 35, 41, 43, 48, 53, 56, 65, 72, 74, 76, 80, 81, 84, 89, 99, 100, 103, 107, 113, 115, 118, 128]\n",
      "x_train:  (90, 128, 128, 1)\n",
      "y_train:  (90, 128, 128, 2)\n",
      "x_val:  (22, 128, 128, 1)\n",
      "y_val:  (22, 128, 128, 2)\n",
      "x_test:  (28, 128, 128, 1)\n",
      "y_test:  (28, 128, 128, 2)\n"
     ]
    }
   ],
   "source": [
    "process_data_sets(use_saved_graphs=True, use_saved_model=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
